{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/pss3e17-binary-classification-pytorch?scriptVersionId=133639630\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport datetime\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,random_split\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-14T20:27:20.099576Z","iopub.execute_input":"2023-06-14T20:27:20.099963Z","iopub.status.idle":"2023-06-14T20:27:20.109006Z","shell.execute_reply.started":"2023-06-14T20:27:20.099933Z","shell.execute_reply":"2023-06-14T20:27:20.107804Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T20:27:22.505091Z","iopub.execute_input":"2023-06-14T20:27:22.50588Z","iopub.status.idle":"2023-06-14T20:27:22.514336Z","shell.execute_reply.started":"2023-06-14T20:27:22.505843Z","shell.execute_reply":"2023-06-14T20:27:22.513298Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s3e17/sample_submission.csv\n/kaggle/input/playground-series-s3e17/train.csv\n/kaggle/input/playground-series-s3e17/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"class MachineFailure(object):\n    \n    def __init__(self,model,loss_fun,optimizer):\n        self.model = model\n        self.loss_fun = loss_fun \n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model.to(self.device)\n        \n        ## Placeholders \n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n        \n        #Variables \n        self.losses =[]\n        self.val_losses = []\n        self.total_epoch = 0\n        \n        #Helper Function\n        self.train_step_fun = self._make_train_step_fun()\n        self.val_step_fun = self._make_val_step_fun()\n        \n    def to(self,device):\n        try:\n            self.device = device\n            self.model.to(self.device)\n            \n        except RuntimeError:\n            self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n            print(f'Could not send it {device}, sending it to {self.device} instead')\n            self.model.to(self.device)\n            \n    \n    def set_loaders(self,train_loader,val_loader=None):\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        \n        \n    def set_tesnorboard(self,name,folder='runs'):\n        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n        \n    def _make_step_train_fun(self):\n        \n        def perform_train_step_fun(X,y):\n            # Set the model to train \n            self.mode.train()\n            #Step 1 - Forward pass / make  predictions\n            yhat = self.model(X)\n            #Step 2 - Compute loss \n            loss = self.loss_fun(yhat,y)\n            #Step 3 - Compute the gradients\n            loss.backward()\n            #Step 4 - Update the variables and set the gradient to 0\n            self.optimizer.step()\n            self.optimizer.zoro_grad()\n            \n            return loss.item()\n        \n        return perform_train_step_fun\n            \n    \n    def _make_step_val_fun(self):\n        \n        def perform_val_step_fun(X):\n            # Set the model to train \n            self.mode.eval()\n            #Step 1 - Forward pass / make  predictions\n            yhat = self.model(X)\n            #Step 2 - Compute loss \n            loss = self.loss_fun(yhat,y)\n            \n            return loss.item()\n        \n        return perform_val_step_fun\n    \n    \n    def _mini_batch(self,validation=False):\n        \n        if validation:\n            data_loader = self.val_loader\n            step_fun = self.val_step_fun\n        else:\n            data_loader = self.train_loader\n            step_fun = self.train_loader\n        \n        if data_loader is None:\n            return None\n        \n        # Loop mini-batch \n        mini_batch_losses =[] \n        \n        for x_batch,y_batch  in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n            \n            mini_batch_loss = step_fun(x_batch,y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n            \n        loss = np.mean(mini_batch_losses)\n        \n        return loss\n    \n    def set_seed(self,seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        \n    \n    def train(self,n_epochs,seed=42):\n        #Reproducibility\n        self.set_seed(seed)\n        \n        for epoch in range(n_epochs):\n            self.total_epoch +=1\n            \n            #inner loop perform training using mini_batch\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n            \n            #Validation \n            with torch.no_grad():\n                #Perform evaluation using mini-batch\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n                \n            #SummaryWriter \n            if self.writer:\n                scalars ={\n                    'training':loss}\n                if val_loss is not None:\n                    scalars.update({'validation':val_loss})\n                    \n                #Record both losses for each epoch\n                self.writer.add_scalars(main_tag='loss',tag_scalar_dict=scalars,global_step=epoch)\n                \n                \n        if self.writer:\n            #Flush the writer \n            self.writer.flush()\n        \n    \n    def save_checkpoint(self,filename):\n        #Build the dictionary with all the elements for resuming training\n        checkpoint = {\n            'epoch':self.total_epoch,\n            'model_state_dict':self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'loss': self.losses,\n            'val_loss': self.val_losses\n        }\n        \n        torch.save(checkpoint,filename)\n        \n    \n     def load_checkpoint(self, filename):\n        # Loads dictionary\n        checkpoint = torch.load(filename)\n\n        # Restore state for model and optimizer\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n        self.total_epochs = checkpoint['epoch']\n        self.losses = checkpoint['loss']\n        self.val_losses = checkpoint['val_loss']\n\n        self.model.train() # always use TRAIN for resuming training   \n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval() \n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label='Training Loss', c='b')\n        plt.plot(self.val_losses, label='Validation Loss', c='r')\n        plt.yscale('log')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.tight_layout()\n        return fig\n\n    def add_graph(self):\n        # Fetches a single mini-batch so we can use add_graph\n        if self.train_loader and self.writer:\n            x_sample, y_sample = next(iter(self.train_loader))\n            self.writer.add_graph(self.model, x_sample.to(self.device))\n        \n        \n        \n    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-15T05:40:51.613104Z","iopub.execute_input":"2023-06-15T05:40:51.61346Z","iopub.status.idle":"2023-06-15T05:40:51.636794Z","shell.execute_reply.started":"2023-06-15T05:40:51.61343Z","shell.execute_reply":"2023-06-15T05:40:51.635862Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
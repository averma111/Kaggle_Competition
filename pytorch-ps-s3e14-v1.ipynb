{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/pytorch-ps-s3e14-v1?scriptVersionId=128612333\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"%%capture\n!pip install flaml","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:23:36.248228Z","iopub.execute_input":"2023-05-07T07:23:36.248614Z","iopub.status.idle":"2023-05-07T07:23:46.978325Z","shell.execute_reply.started":"2023-05-07T07:23:36.248584Z","shell.execute_reply":"2023-05-07T07:23:46.976759Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import random_split","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:23:58.02129Z","iopub.execute_input":"2023-05-07T07:23:58.021804Z","iopub.status.idle":"2023-05-07T07:23:58.029332Z","shell.execute_reply.started":"2023-05-07T07:23:58.021766Z","shell.execute_reply":"2023-05-07T07:23:58.027589Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"class RegressionBlueBerryNNet(torch.nn.Module):\n    def __init__(self,input_features):\n        super(RegressionBlueBerryNNet,self).__init__()\n        self.input_layer = torch.nn.Linear(input_features,1)\n        self.relu = torch.nn.ReLU()\n        \n    def forward(self,inputs):\n        x = self.relu(self.input_layer(inputs))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:25:15.350585Z","iopub.execute_input":"2023-05-07T07:25:15.35105Z","iopub.status.idle":"2023-05-07T07:25:15.359882Z","shell.execute_reply.started":"2023-05-07T07:25:15.351019Z","shell.execute_reply":"2023-05-07T07:25:15.358433Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR='/kaggle/input/playground-series-s3e14'\nORIGINAL_DIR='/kaggle/input/wild-blueberry-yield-prediction/Data in Brief/Data in Brief/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n# Train Data\nclass TrainData(Dataset):\n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:29:05.414329Z","iopub.execute_input":"2023-05-07T07:29:05.414835Z","iopub.status.idle":"2023-05-07T07:29:05.422159Z","shell.execute_reply.started":"2023-05-07T07:29:05.414801Z","shell.execute_reply":"2023-05-07T07:29:05.420918Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def load_data(data_dir=ROOT_DIR,data_original=ORIGINAL_DIR):\n    \n    train_data = TrainData(torch.tensor(data=X_train,dtype=torch.float32,requires_grad=True),\n                       torch.tensor(data=y_train,dtype=torch.float32,requires_grad=True))\n    test_data = TrainData(torch.tensor(data=X_test,dtype=torch.float32,requires_grad=True),\n                       torch.tensor(data=y_test,dtype=torch.float32,requires_grad=True))\n    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(dataset=test_data, batch_size=1)\n    \n    \n    return train_loader,test_loader\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:28:25.36539Z","iopub.execute_input":"2023-05-07T07:28:25.365896Z","iopub.status.idle":"2023-05-07T07:28:25.374202Z","shell.execute_reply.started":"2023-05-07T07:28:25.365861Z","shell.execute_reply":"2023-05-07T07:28:25.372543Z"},"trusted":true},"execution_count":50,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[50], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (3518938345.py, line 2)","output_type":"error"}]},{"cell_type":"code","source":"from ray import tune\n\ndef train_nnet(config, checkpoint_dir=None, data_dir=None):\n    if \"l1\" not in config:\n        logger.warning(config)\n    net = Net(2**config[\"l1\"], 2**config[\"l2\"])\n\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if torch.cuda.device_count() > 1:\n            net = nn.DataParallel(net)\n    net.to(device)\n\n    criterion = nn.L1Loss()\n    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n    if checkpoint_dir:\n        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n        model_state, optimizer_state = torch.load(checkpoint)\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    trainset, testset = load_data(data_dir)\n\n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = random_split(\n        trainset, [test_abs, len(trainset) - test_abs])\n\n    trainloader = torch.utils.data.DataLoader(\n        train_subset,\n        batch_size=int(2**config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=4)\n    valloader = torch.utils.data.DataLoader(\n        val_subset,\n        batch_size=int(2**config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=4)\n\n    for epoch in range(int(round(config[\"num_epochs\"]))):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n                                                running_loss / epoch_steps))\n                running_loss = 0.0\n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        for i, data in enumerate(valloader, 0):\n            with torch.no_grad():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.cpu().numpy()\n                val_steps += 1\n\n        # Here we save a checkpoint. It is automatically registered with\n        # Ray Tune and will potentially be passed as the `checkpoint_dir`\n        # parameter in future iterations.\n        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, \"checkpoint\")\n            torch.save(\n                (net.state_dict(), optimizer.state_dict()), path)\n\n        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n    print(\"Finished Training\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _test_accuracy(net, device=\"cpu\"):\n    trainset, testset = load_data()\n\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=4, shuffle=False, num_workers=2)\n\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport flaml\nimport os\n\ndata_dir = os.path.abspath(\"data\")\nload_data(data_dir)  # Download data for all trials before starting the run","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_num_epoch = 100\nconfig = {\n    \"l1\": tune.randint(2, 9),   # log transformed with base 2\n    \"l2\": tune.randint(2, 9),   # log transformed with base 2\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"num_epochs\": tune.loguniform(1, max_num_epoch),\n    \"batch_size\": tune.randint(1, 5)    # log transformed with base 2\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart_time = time.time()\nresult = flaml.tune.run(\n    tune.with_parameters(train_cifar, data_dir=data_dir),\n    config=config,\n    metric=\"loss\",\n    mode=\"min\",\n    low_cost_partial_config={\"num_epochs\": 1},\n    max_resource=max_num_epoch,\n    min_resource=1,\n    scheduler=\"asha\",  # Use asha scheduler to perform early stopping based on intermediate results reported\n    resources_per_trial={\"cpu\": 1, \"gpu\": gpus_per_trial},\n    local_dir='logs/',\n    num_samples=num_samples,\n    time_budget_s=time_budget_s,\n    use_ray=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"#trials={len(result.trials)}\")\nprint(f\"time={time.time()-start_time}\")\nbest_trial = result.get_best_trial(\"loss\", \"min\", \"all\")\nprint(\"Best trial config: {}\".format(best_trial.config))\nprint(\"Best trial final validation loss: {}\".format(\n    best_trial.metric_analysis[\"loss\"][\"min\"]))\nprint(\"Best trial final validation accuracy: {}\".format(\n    best_trial.metric_analysis[\"accuracy\"][\"max\"]))\n\nbest_trained_model = Net(2**best_trial.config[\"l1\"],\n                         2**best_trial.config[\"l2\"])\ndevice = \"cpu\"\nif torch.cuda.is_available():\n    device = \"cuda:0\"\n    if gpus_per_trial > 1:\n        best_trained_model = nn.DataParallel(best_trained_model)\nbest_trained_model.to(device)\n\ncheckpoint_value = getattr(best_trial.checkpoint, \"dir_or_data\", None) or best_trial.checkpoint.value\ncheckpoint_path = os.path.join(checkpoint_value, \"checkpoint\")\n\nmodel_state, optimizer_state = torch.load(checkpoint_path)\nbest_trained_model.load_state_dict(model_state)\n\ntest_acc = _test_accuracy(best_trained_model, device)\nprint(\"Best trial test set accuracy: {}\".format(test_acc))","metadata":{},"execution_count":null,"outputs":[]}]}
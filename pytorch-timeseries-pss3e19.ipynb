{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/lightning-timeseries?scriptVersionId=136718419\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Re-loads all imports every time the cell is ran. \n%reload_ext autoreload\n\nfrom time import time\n\nimport numpy as np\nimport pandas as pd\npd.options.display.float_format = '{:,.5f}'.format\n\nfrom IPython.display import display\n\n# Sklearn tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Neural Networks\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers.csv_logs import CSVLogger\n\n# Plotting\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualizations\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:10.040286Z","iopub.execute_input":"2023-07-14T02:51:10.040683Z","iopub.status.idle":"2023-07-14T02:51:10.141061Z","shell.execute_reply.started":"2023-07-14T02:51:10.040653Z","shell.execute_reply":"2023-07-14T02:51:10.139713Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s3e19/sample_submission.csv\n/kaggle/input/playground-series-s3e19/train.csv\n/kaggle/input/playground-series-s3e19/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"df1 = train_df.groupby(by=[\"country\"]).size().reset_index(name=\"counts\")\ndf2 = train_df.groupby(by=[\"store\"]).size().reset_index(name=\"counts\")\ndf3 = train_df.groupby(by=[\"product\"]).size().reset_index(name=\"counts\")\n\nfig = px.pie(df1, values='counts', names='country',\n             title='Countries',\n             width=800, height=500)\n\nfig.update_traces(textposition='inside', textinfo='value+label')\nfig.show()\n\nfig = px.pie(df2, values='counts', names='store',\n             title='Stores',\n             width=800, height=500)\n\nfig.update_traces(textposition='inside', textinfo='value+label')\nfig.show()\n\nfig = px.pie(df3, values='counts', names='product',\n             title='Products',\n             width=800, height=500)\n\nfig.update_traces(textposition='inside', textinfo='value')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_sold =  train_df.groupby(by='country')['num_sold'].sum()\n\nfig = px.pie(train_df, values='num_sold', names='country',color='country',\n             title='Number of units sold w.r.t country',\n             width=800, height=500)\nfig.show()\n\n\ncountry_df = train_df.groupby([\"date\",\"country\"])[\"num_sold\"].sum().reset_index()\nplt = px.line(country_df, x=\"date\", y=\"num_sold\", color=\"country\", height=480)\n\nplt.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n))\n\nplt.show()\n\n# ------------------------------------------------\n\nnum_sold =  train_df.groupby(by='product')['num_sold'].sum()\nfig = px.pie(train_df, values='num_sold', names='product',color='product',\n             title='Number of units sold w.r.t product',\n             width=800, height=500)\nfig.show()\n\nproduct_df = train_df.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()\nplt = px.line(product_df, x=\"date\", y=\"num_sold\", color=\"product\", height=500)\nplt.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\n\nplt.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n))\n\nplt.show()\n\n#------------------------------------------------\n\nnum_sold =  train_df.groupby(by='store')['num_sold'].sum()\n\nfig = px.pie(train_df, values='num_sold', names='store',color='store',\n             title='Number of units sold w.r.t store',\n             width=800, height=500)\nfig.show()\n\nstore_df = train_df.groupby([\"date\",\"store\"])[\"num_sold\"].sum().reset_index()\nplt = px.line(store_df, x=\"date\", y=\"num_sold\", color=\"store\", height=480)\nplt.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_df = train_df.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n\nplt = px.line(agg_df, x=\"date\", y=\"num_sold\",height=500,title='Daily Data')\nplt.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n\n))\n# plt.update_xaxes(rangeslider_visible=True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekly_df = agg_df.groupby([pd.Grouper(key=\"date\", freq=\"W\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()\nmonthly_df = agg_df.groupby([pd.Grouper(key=\"date\", freq=\"MS\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()\n# agg_df = weekly_df.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n\nplt = px.line(weekly_df[1:-1], x=\"date\", y=\"num_sold\",height=500,title='Weekly Data')\nplt.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n\n))\n# plt.update_xaxes(rangeslider_visible=True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekly_df.head()\n\n# agg_df = monthly_df.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n\nplt = px.line(monthly_df[1:-1], x=\"date\", y=\"num_sold\",height=500, title='Monthly Data')\nplt.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n))\n# plt.update_xaxes(rangeslider_visible=True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_df['moving_average'] = agg_df['num_sold'].rolling(30).mean()\nfig = px.line(agg_df, x=\"date\", y=[\"num_sold\",\"moving_average\"], title = \"\")\n\nfig.add_hline(y=agg_df['num_sold'].mean(), line_dash=\"dot\",\n              annotation_text=\"Average Sales\", \n              annotation_position=\"bottom right\")\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.98,\n    xanchor=\"left\",\n    x=0.01,\n#     bgcolor=\"LightSteelBlue\",\n))\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nresults = seasonal_decompose(agg_df['num_sold'], model='additive', period=365)\n\nfig_2 = px.line(y=results.trend, x=results.trend.index, title='Trend',width=850, height=400)\nfig_2.show()\nfig_3 = px.line(y=results.seasonal, x=results.seasonal.index, title='Seasonality',width=850, height=400)\nfig_3.show()\nfig_4 = px.line(y=results.resid, x=results.resid.index, title='Residual',width=850, height=400)\nfig_4.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeseriesDataset(Dataset):   \n    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n        self.X = torch.tensor(X).float()\n        self.y = torch.tensor(y).float()\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return self.X.__len__() - (self.seq_len-1)\n\n    def __getitem__(self, index):\n        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:10.142946Z","iopub.execute_input":"2023-07-14T02:51:10.143237Z","iopub.status.idle":"2023-07-14T02:51:10.154273Z","shell.execute_reply.started":"2023-07-14T02:51:10.143211Z","shell.execute_reply":"2023-07-14T02:51:10.152805Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"class TimeseriesDatasetTest(Dataset):   \n    def __init__(self, X: np.ndarray, seq_len: int = 1):\n        self.X = torch.tensor(X).float()\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return self.X.__len__() - (self.seq_len-1)\n\n    def __getitem__(self, index):\n        return (self.X[index:index+self.seq_len])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:10.155659Z","iopub.execute_input":"2023-07-14T02:51:10.155953Z","iopub.status.idle":"2023-07-14T02:51:10.172635Z","shell.execute_reply.started":"2023-07-14T02:51:10.155927Z","shell.execute_reply":"2023-07-14T02:51:10.17124Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"class FeatureGeneration:\n    \n    def datetime_features(self,df):\n        df['month'] = df['date'].dt.month\n        df['day'] = df['date'].dt.day\n        df['year'] = df['date'].dt.year\n        df['dayofweek'] = df['date'].dt.dayofweek\n        df['quarter'] = df['date'].dt.quarter\n        df['dayofmonth'] = df['date'].dt.day\n        df['weekofyear'] = df['date'].dt.weekofyear\n        return df\n    \n    def seasonality_features(self,df):\n        df['month_sin'] = np.sin(2*np.pi*df.month/12)\n        df['month_cos'] = np.cos(2*np.pi*df.month/12)\n        df['day_sin'] = np.sin(2*np.pi*df.day/24)\n        df['day_cos'] = np.cos(2*np.pi*df.day/24)\n        return df\n    \nfg =FeatureGeneration()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:10.175244Z","iopub.execute_input":"2023-07-14T02:51:10.175681Z","iopub.status.idle":"2023-07-14T02:51:10.188872Z","shell.execute_reply.started":"2023-07-14T02:51:10.175653Z","shell.execute_reply":"2023-07-14T02:51:10.187842Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nclass ForecastingDataModule(pl.LightningDataModule):\n    \n    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n        super().__init__()\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.X_train = None\n        self.y_train = None\n        self.X_val = None\n        self.y_val = None\n        self.X_test = None\n        self.X_test = None\n        self.columns = None\n        self.preprocessing = None\n\n    def prepare_data(self):\n        path = '/kaggle/input/playground-series-s3e19/train.csv'\n        df = pd.read_csv(\n            path, \n            sep=',', \n            parse_dates=['date'], \n            infer_datetime_format=True, \n            low_memory=False\n        )\n        \n        X = df.copy()\n        X = fg.datetime_features(df)\n        X = fg.seasonality_features(X)\n        X['store'] =  LabelEncoder().fit_transform(X['store'])\n        X['product'] =  LabelEncoder().fit_transform(X['product'])\n        X['country'] =  LabelEncoder().fit_transform(X['country'])\n        X = X.loc[:, X.columns!='id']\n        y = X['num_sold']\n        X.drop(columns=['date','num_sold'],axis=1,inplace=True)\n        self.columns = X.columns\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size=0.25, shuffle=False\n        )\n        #print(X_train.shape,X_val.shape,y_train.shape,y_val.shape)\n        self.X_train=X_train\n        self.X_val = X_val\n        self.y_train = y_train\n        self.y_val = y_val\n\n    def setup(self, stage=None):\n        preprocessing = StandardScaler()\n        if stage == 'fit':\n            self.X_train = preprocessing.fit_transform(self.X_train)\n            self.y_train = self.y_train.values\n            self.X_val = preprocessing.fit_transform(self.X_val)\n            self.y_val = self.y_val.values\n\n        if stage == 'test':\n            self.X_test = preprocessing.fit_transform(self.X_test)\n            \n        \n\n    def train_dataloader(self):\n        train_dataset = TimeseriesDataset(self.X_train, \n                                          self.y_train, \n                                          seq_len=self.seq_len)\n        train_loader = DataLoader(train_dataset, \n                                  batch_size = self.batch_size, \n                                  shuffle = False, \n                                  num_workers = self.num_workers)\n        \n        return train_loader\n\n    def val_dataloader(self):\n        val_dataset = TimeseriesDataset(self.X_val, \n                                        self.y_val, \n                                        seq_len=self.seq_len)\n        val_loader = DataLoader(val_dataset, \n                                batch_size = self.batch_size, \n                                shuffle = False, \n                                num_workers = self.num_workers)\n\n        return val_loader\n    \n    \n    def prepare_data_test(self):\n        path = '/kaggle/input/playground-series-s3e19/test.csv'\n        df = pd.read_csv(\n            path, \n            sep=',', \n            parse_dates=['date'], \n            infer_datetime_format=True, \n            low_memory=False\n        )\n        \n        X = df.copy()\n        X = fg.datetime_features(df)\n        X = fg.seasonality_features(X)\n        X['store'] =  LabelEncoder().fit_transform(X['store'])\n        X['product'] =  LabelEncoder().fit_transform(X['product'])\n        X['country'] =  LabelEncoder().fit_transform(X['country'])\n        X = X.loc[:, X.columns!='id']\n        X.drop(columns=['date'],axis=1,inplace=True)\n        self.columns = X.columns\n        self.X_test = X\n\n    def test_dataloader(self):\n        test_dataset = TimeseriesDatasetTest(self.X_test,\n                                         seq_len=self.seq_len)\n        test_loader = DataLoader(test_dataset, \n                                 batch_size = self.batch_size, \n                                 shuffle = False, \n                                 num_workers = self.num_workers)\n\n        return test_loader","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:10.190169Z","iopub.execute_input":"2023-07-14T02:51:10.190565Z","iopub.status.idle":"2023-07-14T02:51:10.216263Z","shell.execute_reply.started":"2023-07-14T02:51:10.190532Z","shell.execute_reply":"2023-07-14T02:51:10.215212Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"class LSTMRegressor(pl.LightningModule):\n    def __init__(self, \n                 n_features, \n                 hidden_size, \n                 seq_len, \n                 batch_size,\n                 num_layers, \n                 dropout, \n                 learning_rate,\n                 criterion):\n        super(LSTMRegressor, self).__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.criterion = criterion\n        self.learning_rate = learning_rate\n\n        self.lstm = nn.LSTM(input_size=n_features, \n                            hidden_size=hidden_size,\n                            num_layers=num_layers, \n                            dropout=dropout, \n                            batch_first=True)\n        self.linear = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        y_pred = self.linear(lstm_out[:,-1])\n        return y_pred\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('Train_loss', loss)\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('Validation_loss', loss)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:30.715846Z","iopub.execute_input":"2023-07-14T02:51:30.71643Z","iopub.status.idle":"2023-07-14T02:51:30.729968Z","shell.execute_reply.started":"2023-07-14T02:51:30.716399Z","shell.execute_reply":"2023-07-14T02:51:30.729228Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"p = dict(\n    seq_len = 24,\n    batch_size = 32, \n    criterion = nn.MSELoss(),\n    max_epochs = 10,\n    n_features = 14,\n    hidden_size =50,\n    num_layers = 4,\n    dropout = 0.2,\n    learning_rate = 0.0001,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:31.246854Z","iopub.execute_input":"2023-07-14T02:51:31.248162Z","iopub.status.idle":"2023-07-14T02:51:31.257962Z","shell.execute_reply.started":"2023-07-14T02:51:31.248112Z","shell.execute_reply":"2023-07-14T02:51:31.256324Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"seed_everything(1)\ncsv_logger = CSVLogger('./', name='lstm', version='0'),\n\nmodel = LSTMRegressor(\n    n_features = p['n_features'],\n    hidden_size = p['hidden_size'],\n    seq_len = p['seq_len'],\n    batch_size = p['batch_size'],\n    criterion = p['criterion'],\n    num_layers = p['num_layers'],\n    dropout = p['dropout'],\n    learning_rate = p['learning_rate']\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:32.000681Z","iopub.execute_input":"2023-07-14T02:51:32.001023Z","iopub.status.idle":"2023-07-14T02:51:32.014963Z","shell.execute_reply.started":"2023-07-14T02:51:32.000995Z","shell.execute_reply":"2023-07-14T02:51:32.013372Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:33.179885Z","iopub.execute_input":"2023-07-14T02:51:33.180453Z","iopub.status.idle":"2023-07-14T02:51:33.188978Z","shell.execute_reply.started":"2023-07-14T02:51:33.180424Z","shell.execute_reply":"2023-07-14T02:51:33.188061Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"LSTMRegressor(\n  (criterion): MSELoss()\n  (lstm): LSTM(14, 50, num_layers=4, batch_first=True, dropout=0.2)\n  (linear): Linear(in_features=50, out_features=1, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    max_epochs=p['max_epochs'],\n    logger=csv_logger,\n    accelerator='auto',\n    log_every_n_steps=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:39.031264Z","iopub.execute_input":"2023-07-14T02:51:39.032376Z","iopub.status.idle":"2023-07-14T02:51:39.100119Z","shell.execute_reply.started":"2023-07-14T02:51:39.032324Z","shell.execute_reply":"2023-07-14T02:51:39.09896Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"dm = ForecastingDataModule(\n    seq_len = p['seq_len'],\n    batch_size = p['batch_size']\n)\ndm.prepare_data()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:40.489918Z","iopub.execute_input":"2023-07-14T02:51:40.49031Z","iopub.status.idle":"2023-07-14T02:51:40.773143Z","shell.execute_reply.started":"2023-07-14T02:51:40.490259Z","shell.execute_reply":"2023-07-14T02:51:40.772103Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model, dm)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:44.881706Z","iopub.execute_input":"2023-07-14T02:51:44.88267Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5475dde20484bb88328d077defd4e3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"metrics = pd.read_csv('./lstm/0/metrics.csv')\ntrain_loss = metrics[['Train_loss', 'step', 'epoch']][~np.isnan(metrics['Train_loss'])]\nval_loss = metrics[['Validation_loss', 'epoch']][~np.isnan(metrics['Validation_loss'])]\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\naxes[0].set_title('Train loss per batch')\naxes[0].plot(train_loss['step'], train_loss['Train_loss'])\naxes[1].set_title('Validation loss per epoch')\naxes[1].plot(val_loss['epoch'], val_loss['Validation_loss'], color='orange')\nplt.show(block = True)\n\nprint('MSE:')\nprint(f\"Train loss: {train_loss['Train_loss'].iloc[-1]:.3f}\")\nprint(f\"Val loss:   {val_loss['Validation_loss'].iloc[-1]:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dm.prepare_data_test()\ntrainer.predict(model, datamodule=dm)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T02:51:10.335399Z","iopub.status.idle":"2023-07-14T02:51:10.33582Z","shell.execute_reply.started":"2023-07-14T02:51:10.335608Z","shell.execute_reply":"2023-07-14T02:51:10.33563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
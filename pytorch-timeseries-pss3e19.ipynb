{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/lightning-timeseries?scriptVersionId=136707965\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Re-loads all imports every time the cell is ran. \n%reload_ext autoreload\n\nfrom time import time\n\nimport numpy as np\nimport pandas as pd\npd.options.display.float_format = '{:,.5f}'.format\n\nfrom IPython.display import display\n\n# Sklearn tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Neural Networks\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers.csv_logs import CSVLogger\n\n# Plotting\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:26.025094Z","iopub.execute_input":"2023-07-13T23:26:26.025592Z","iopub.status.idle":"2023-07-13T23:26:51.634145Z","shell.execute_reply.started":"2023-07-13T23:26:26.025551Z","shell.execute_reply":"2023-07-13T23:26:51.632608Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/playground-series-s3e19/sample_submission.csv\n/kaggle/input/playground-series-s3e19/train.csv\n/kaggle/input/playground-series-s3e19/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"class TimeseriesDataset(Dataset):   \n    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n        self.X = torch.tensor(X).float()\n        self.y = torch.tensor(y).float()\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return self.X.__len__() - (self.seq_len-1)\n\n    def __getitem__(self, index):\n        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:51.636569Z","iopub.execute_input":"2023-07-13T23:26:51.63701Z","iopub.status.idle":"2023-07-13T23:26:51.650297Z","shell.execute_reply.started":"2023-07-13T23:26:51.636972Z","shell.execute_reply":"2023-07-13T23:26:51.64835Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nclass ForecastingDataModule(pl.LightningDataModule):\n    \n    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n        super().__init__()\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.X_train = None\n        self.y_train = None\n        self.X_val = None\n        self.y_val = None\n        self.X_test = None\n        self.X_test = None\n        self.columns = None\n        self.preprocessing = None\n\n    def prepare_data(self):\n        path = '/kaggle/input/playground-series-s3e19/train.csv'\n        df = pd.read_csv(\n            path, \n            sep=',', \n            parse_dates=['date'], \n            infer_datetime_format=True, \n            low_memory=False\n        )\n\n        X = df.copy()\n        X['store'] =  LabelEncoder().fit_transform(X['store'])\n        X['product'] =  LabelEncoder().fit_transform(X['product'])\n        X['country'] =  LabelEncoder().fit_transform(X['country'])\n        X = X.loc[:, X.columns!='id']\n        y = X['num_sold']\n        X.drop(columns=['date','num_sold'],axis=1,inplace=True)\n        self.columns = X.columns\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size=0.25, shuffle=False\n        )\n        #print(X_train.shape,X_val.shape,y_train.shape,y_val.shape)\n        self.X_train=X_train\n        self.X_val = X_val\n        self.y_train = y_train\n        self.y_val = y_val\n\n    def setup(self, stage=None):\n        preprocessing = StandardScaler()\n        if stage == 'fit':\n            self.X_train = preprocessing.fit_transform(self.X_train)\n            self.y_train = self.y_train.values\n            self.X_val = preprocessing.fit_transform(self.X_val)\n            self.y_val = self.y_val.values\n\n        if stage == 'test':\n            self.X_test = preprocessing.fit_transform(X_test)\n            \n        \n\n    def train_dataloader(self):\n        train_dataset = TimeseriesDataset(self.X_train, \n                                          self.y_train, \n                                          seq_len=self.seq_len)\n        train_loader = DataLoader(train_dataset, \n                                  batch_size = self.batch_size, \n                                  shuffle = False, \n                                  num_workers = self.num_workers)\n        \n        return train_loader\n\n    def val_dataloader(self):\n        val_dataset = TimeseriesDataset(self.X_val, \n                                        self.y_val, \n                                        seq_len=self.seq_len)\n        val_loader = DataLoader(val_dataset, \n                                batch_size = self.batch_size, \n                                shuffle = False, \n                                num_workers = self.num_workers)\n\n        return val_loader\n    \n    \n    def prepare_data_test(self):\n        path = '/kaggle/input/playground-series-s3e19/test.csv'\n        df = pd.read_csv(\n            path, \n            sep=',', \n            parse_dates=['date'], \n            infer_datetime_format=True, \n            low_memory=False\n        )\n\n        X = df.copy()\n        X['store'] =  LabelEncoder().fit_transform(X['store'])\n        X['product'] =  LabelEncoder().fit_transform(X['product'])\n        X['country'] =  LabelEncoder().fit_transform(X['country'])\n        X = X.loc[:, X.columns!='id']\n        X.drop(columns=['date'],axis=1,inplace=True)\n        self.columns = X.columns\n        self.X_test = X\n\n    def test_dataloader(self):\n        test_dataset = TimeseriesDataset(self.X_test, \n                                         self.y_test, \n                                         seq_len=self.seq_len)\n        test_loader = DataLoader(test_dataset, \n                                 batch_size = self.batch_size, \n                                 shuffle = False, \n                                 num_workers = self.num_workers)\n\n        return test_loader","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:51.652072Z","iopub.execute_input":"2023-07-13T23:26:51.653381Z","iopub.status.idle":"2023-07-13T23:26:51.67919Z","shell.execute_reply.started":"2023-07-13T23:26:51.65331Z","shell.execute_reply":"2023-07-13T23:26:51.677629Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class LSTMRegressor(pl.LightningModule):\n    def __init__(self, \n                 n_features, \n                 hidden_size, \n                 seq_len, \n                 batch_size,\n                 num_layers, \n                 dropout, \n                 learning_rate,\n                 criterion):\n        super(LSTMRegressor, self).__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.criterion = criterion\n        self.learning_rate = learning_rate\n\n        self.lstm = nn.LSTM(input_size=n_features, \n                            hidden_size=hidden_size,\n                            num_layers=num_layers, \n                            dropout=dropout, \n                            batch_first=True)\n        self.linear = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        y_pred = self.linear(lstm_out[:,-1])\n        return y_pred\n    \n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=self.learning_rate,momentum=0.9)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('Train_loss', loss)\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('Validation_loss', loss)\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('Test_loss', loss)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:51.683343Z","iopub.execute_input":"2023-07-13T23:26:51.68406Z","iopub.status.idle":"2023-07-13T23:26:51.706855Z","shell.execute_reply.started":"2023-07-13T23:26:51.683997Z","shell.execute_reply":"2023-07-13T23:26:51.705327Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"p = dict(\n    seq_len = 24,\n    batch_size = 64, \n    criterion = nn.MSELoss(),\n    max_epochs = 10,\n    n_features = 3,\n    hidden_size = 100,\n    num_layers = 1,\n    dropout = 0.2,\n    learning_rate = 0.001,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:51.708503Z","iopub.execute_input":"2023-07-13T23:26:51.708937Z","iopub.status.idle":"2023-07-13T23:26:51.729931Z","shell.execute_reply.started":"2023-07-13T23:26:51.708897Z","shell.execute_reply":"2023-07-13T23:26:51.728638Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"seed_everything(1)\ncsv_logger = CSVLogger('./', name='lstm', version='0'),\n\nmodel = LSTMRegressor(\n    n_features = p['n_features'],\n    hidden_size = p['hidden_size'],\n    seq_len = p['seq_len'],\n    batch_size = p['batch_size'],\n    criterion = p['criterion'],\n    num_layers = p['num_layers'],\n    dropout = p['dropout'],\n    learning_rate = p['learning_rate']\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:51.731659Z","iopub.execute_input":"2023-07-13T23:26:51.732755Z","iopub.status.idle":"2023-07-13T23:26:51.775247Z","shell.execute_reply.started":"2023-07-13T23:26:51.732713Z","shell.execute_reply":"2023-07-13T23:26:51.774262Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:51.776717Z","iopub.execute_input":"2023-07-13T23:26:51.777554Z","iopub.status.idle":"2023-07-13T23:26:51.785481Z","shell.execute_reply.started":"2023-07-13T23:26:51.777515Z","shell.execute_reply":"2023-07-13T23:26:51.784238Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"LSTMRegressor(\n  (criterion): MSELoss()\n  (lstm): LSTM(3, 100, batch_first=True, dropout=0.2)\n  (linear): Linear(in_features=100, out_features=1, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    max_epochs=p['max_epochs'],\n    logger=csv_logger,\n    accelerator='auto',\n    log_every_n_steps=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:51.78721Z","iopub.execute_input":"2023-07-13T23:26:51.787821Z","iopub.status.idle":"2023-07-13T23:26:52.525842Z","shell.execute_reply.started":"2023-07-13T23:26:51.787785Z","shell.execute_reply":"2023-07-13T23:26:52.524857Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dm = ForecastingDataModule(\n    seq_len = p['seq_len'],\n    batch_size = p['batch_size']\n)\ndm.prepare_data()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:52.527686Z","iopub.execute_input":"2023-07-13T23:26:52.528535Z","iopub.status.idle":"2023-07-13T23:26:53.053552Z","shell.execute_reply.started":"2023-07-13T23:26:52.528491Z","shell.execute_reply":"2023-07-13T23:26:53.052029Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model, dm)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T23:26:53.057302Z","iopub.execute_input":"2023-07-13T23:26:53.057702Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a79913ec2164ef6be709f2b52d8f189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"metrics = pd.read_csv('./lstm/0/metrics.csv')\ntrain_loss = metrics[['Train_loss', 'step', 'epoch']][~np.isnan(metrics['Train_loss'])]\nval_loss = metrics[['Validation_loss', 'epoch']][~np.isnan(metrics['Validation_loss'])]\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\naxes[0].set_title('Train loss per batch')\naxes[0].plot(train_loss['step'], train_loss['Train_loss'])\naxes[1].set_title('Validation loss per epoch')\naxes[1].plot(val_loss['epoch'], val_loss['Validation_loss'], color='orange')\nplt.show(block = True)\n\nprint('MSE:')\nprint(f\"Train loss: {train_loss['Train_loss'].iloc[-1]:.3f}\")\nprint(f\"Val loss:   {val_loss['Validation_loss'].iloc[-1]:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dm.prepare_data_test()\ntrainer.test(model, datamodule=dm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = pd.read_csv('./lstm/0/metrics.csv')\ntest_loss = metrics['Test_loss'].iloc[-1]\n\nfig, axes = plt.subplots(1, 1, figsize=(16, 5), dpi=100)\naxes[0].set_title('Test loss per batch')\naxes[0].plot(test_loss['step'], test_loss['Test_loss'])\nplt.show(block = True)\n\nprint('MSE:')\nprint(f'Test loss: {test_loss:.3f}')","metadata":{},"execution_count":null,"outputs":[]}]}
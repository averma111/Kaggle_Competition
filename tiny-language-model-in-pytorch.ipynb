{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/language-model-in-pytorch?scriptVersionId=130151740\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torchtext\nimport datasets\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-19T06:50:17.065605Z","iopub.execute_input":"2023-05-19T06:50:17.066015Z","iopub.status.idle":"2023-05-19T06:50:21.499465Z","shell.execute_reply.started":"2023-05-19T06:50:17.065977Z","shell.execute_reply":"2023-05-19T06:50:21.498473Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:21.502034Z","iopub.execute_input":"2023-05-19T06:50:21.503004Z","iopub.status.idle":"2023-05-19T06:50:21.544117Z","shell.execute_reply.started":"2023-05-19T06:50:21.502968Z","shell.execute_reply":"2023-05-19T06:50:21.543443Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x79f99680df30>"},"metadata":{}}]},{"cell_type":"code","source":"dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\nprint(dataset)\nprint(dataset['train'][88]['text'])","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:21.545428Z","iopub.execute_input":"2023-05-19T06:50:21.546411Z","iopub.status.idle":"2023-05-19T06:50:24.259099Z","shell.execute_reply.started":"2023-05-19T06:50:21.546364Z","shell.execute_reply":"2023-05-19T06:50:24.258163Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefc76799d3b404e9ec74337640e7214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50286835abd64060914b1053b0fd1faf"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e1b66516c04714a45fef0f6c9271fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8880d208e0ee4f2e9c84a64899d17698"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . Gunsmiths were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen 'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\ntokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \ntokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \nfn_kwargs={'tokenizer': tokenizer})\nprint(tokenized_dataset['train'][88]['tokens'])","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:24.262153Z","iopub.execute_input":"2023-05-19T06:50:24.262808Z","iopub.status.idle":"2023-05-19T06:50:30.449419Z","shell.execute_reply.started":"2023-05-19T06:50:24.262775Z","shell.execute_reply":"2023-05-19T06:50:30.448349Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4358 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c11609606ee4ef48c60779da3942398"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36718 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934ec5d3cf8a4c3db6a7d0f4aa0fd4cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3760 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1402c190ec64defb25c44a55ecf2925"}},"metadata":{}},{"name":"stdout","text":"['this', 'ammunition', ',', 'and', 'that', 'which', 'i', 'brought', 'with', 'me', ',', 'was', 'rapidly', 'prepared', 'for', 'use', 'at', 'the', 'laboratory', 'established', 'at', 'the', 'little', 'rock', 'arsenal', 'for', 'that', 'purpose', '.', 'as', 'illustrating', 'as', 'the', 'pitiful', 'scarcity', 'of', 'material', 'in', 'the', 'country', ',', 'the', 'fact', 'may', 'be', 'stated', 'that', 'it', 'was', 'found', 'necessary', 'to', 'use', 'public', 'documents', 'of', 'the', 'state', 'library', 'for', 'cartridge', 'paper', '.', 'gunsmiths', 'were', 'employed', 'or', 'conscripted', ',', 'tools', 'purchased', 'or', 'impressed', ',', 'and', 'the', 'repair', 'of', 'the', 'damaged', 'guns', 'i', 'brought', 'with', 'me', 'and', 'about', 'an', 'equal', 'number', 'found', 'at', 'little', 'rock', 'commenced', 'at', 'once', '.', 'but', ',', 'after', 'inspecting', 'the', 'work', 'and', 'observing', 'the', 'spirit', 'of', 'the', 'men', 'i', 'decided', 'that', 'a', 'garrison', '500', 'strong', 'could', 'hold', 'out', 'against', 'fitch', 'and', 'that', 'i', 'would', 'lead', 'the', 'remainder', '-', 'about', '1500', '-', 'to', 'gen', \"'\", 'l', 'rust', 'as', 'soon', 'as', 'shotguns', 'and', 'rifles', 'could', 'be', 'obtained', 'from', 'little', 'rock', 'instead', 'of', 'pikes', 'and', 'lances', ',', 'with', 'which', 'most', 'of', 'them', 'were', 'armed', '.', 'two', 'days', 'elapsed', 'before', 'the', 'change', 'could', 'be', 'effected', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'],min_freq=3) \nvocab.insert_token('<unk>', 0)           \nvocab.insert_token('<eos>', 1)            \nvocab.set_default_index(vocab['<unk>'])   \nprint(len(vocab))                         \nprint(vocab.get_itos()[:10])  ","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:30.451215Z","iopub.execute_input":"2023-05-19T06:50:30.451632Z","iopub.status.idle":"2023-05-19T06:50:34.136022Z","shell.execute_reply.started":"2023-05-19T06:50:30.451599Z","shell.execute_reply":"2023-05-19T06:50:34.135Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"29473\n['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_data(dataset, vocab, batch_size):\n    data = []                                                   \n    for example in dataset:\n        if example['tokens']:                                      \n            tokens = example['tokens'].append('<eos>')             \n            tokens = [vocab[token] for token in example['tokens']] \n            data.extend(tokens)                                    \n    data = torch.LongTensor(data)                                 \n    num_batches = data.shape[0] // batch_size \n    data = data[:num_batches * batch_size]                       \n    data = data.view(batch_size, num_batches)          \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:34.137335Z","iopub.execute_input":"2023-05-19T06:50:34.138161Z","iopub.status.idle":"2023-05-19T06:50:34.146162Z","shell.execute_reply.started":"2023-05-19T06:50:34.138127Z","shell.execute_reply":"2023-05-19T06:50:34.144846Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\ntrain_data = get_data(tokenized_dataset['train'], vocab, batch_size)\nvalid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\ntest_data = get_data(tokenized_dataset['test'], vocab, batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:34.147636Z","iopub.execute_input":"2023-05-19T06:50:34.147972Z","iopub.status.idle":"2023-05-19T06:50:42.317286Z","shell.execute_reply.started":"2023-05-19T06:50:34.147941Z","shell.execute_reply":"2023-05-19T06:50:42.316141Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n        if tie_weights:\n            assert embedding_dim == hidden_dim, 'If tying weights then embedding_dim must equal hidden_dim'\n            self.embedding.weight = self.fc.weight\n        self.init_weights()\n\n    def forward(self, src, hidden):\n        embedding = self.dropout(self.embedding(src))\n        output, hidden = self.lstm(embedding, hidden)          \n        output = self.dropout(output) \n        prediction = self.fc(output)\n        return prediction, hidden\n\n    def init_weights(self):\n        init_range_emb = 0.1\n        init_range_other = 1/math.sqrt(self.hidden_dim)\n        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n        self.fc.bias.data.zero_()\n        for i in range(self.num_layers):\n            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n\n    def init_hidden(self, batch_size, device):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        return hidden, cell\n\n    # We don't learn the hidden state so we can detach it from the computation graph\n    def detach_hidden(self, hidden):\n        hidden, cell = hidden\n        hidden = hidden.detach()\n        cell = cell.detach()\n        return hidden, cell\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:55:57.047618Z","iopub.execute_input":"2023-05-19T06:55:57.04801Z","iopub.status.idle":"2023-05-19T06:55:57.065254Z","shell.execute_reply.started":"2023-05-19T06:55:57.047972Z","shell.execute_reply":"2023-05-19T06:55:57.064069Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 1024             # 400 in the paper\nhidden_dim = 1024                # 1150 in the paper\nnum_layers = 2                   # 3 in the paper\ndropout_rate = 0.65              \ntie_weights = True                  \nlr = 1e-3                        # They used 30 and a different optimizer","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:55:57.469105Z","iopub.execute_input":"2023-05-19T06:55:57.469512Z","iopub.status.idle":"2023-05-19T06:55:57.475455Z","shell.execute_reply.started":"2023-05-19T06:55:57.469473Z","shell.execute_reply":"2023-05-19T06:55:57.474316Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'The model has {num_params:,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:55:57.913655Z","iopub.execute_input":"2023-05-19T06:55:57.914052Z","iopub.status.idle":"2023-05-19T06:55:59.222477Z","shell.execute_reply.started":"2023-05-19T06:55:57.914015Z","shell.execute_reply":"2023-05-19T06:55:59.221487Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"The model has 47,003,425 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_batch(data, seq_len, num_batches, idx):\n    src = data[:, idx:idx+seq_len]                   \n    target = data[:, idx+1:idx+seq_len+1]             \n    return src, target","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:55:59.224863Z","iopub.execute_input":"2023-05-19T06:55:59.225233Z","iopub.status.idle":"2023-05-19T06:55:59.231431Z","shell.execute_reply.started":"2023-05-19T06:55:59.225198Z","shell.execute_reply":"2023-05-19T06:55:59.230325Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n    \n    epoch_loss = 0\n    model.train()\n    # drop all batches that are not a multiple of seq_len\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = model.init_hidden(batch_size, device)\n    \n    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n        optimizer.zero_grad()\n        hidden = model.detach_hidden(hidden)\n\n        src, target = get_batch(data, seq_len, num_batches, idx)\n        src, target = src.to(device), target.to(device)\n        batch_size = src.shape[0]\n        prediction, hidden = model(src, hidden)               \n\n        prediction = prediction.reshape(batch_size * seq_len, -1)   \n        target = target.reshape(-1)\n        loss = criterion(prediction, target)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:56:19.006235Z","iopub.execute_input":"2023-05-19T06:56:19.007066Z","iopub.status.idle":"2023-05-19T06:56:19.016759Z","shell.execute_reply.started":"2023-05-19T06:56:19.007032Z","shell.execute_reply":"2023-05-19T06:56:19.015623Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, data, criterion, batch_size, seq_len, device):\n\n    epoch_loss = 0\n    model.eval()\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = model.init_hidden(batch_size, device)\n\n    with torch.no_grad():\n        for idx in range(0, num_batches - 1, seq_len):\n            hidden = model.detach_hidden(hidden)\n            src, target = get_batch(data, seq_len, num_batches, idx)\n            src, target = src.to(device), target.to(device)\n            batch_size= src.shape[0]\n\n            prediction, hidden = model(src, hidden)\n            prediction = prediction.reshape(batch_size * seq_len, -1)\n            target = target.reshape(-1)\n\n            loss = criterion(prediction, target)\n            epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:56:19.550184Z","iopub.execute_input":"2023-05-19T06:56:19.550586Z","iopub.status.idle":"2023-05-19T06:56:19.560364Z","shell.execute_reply.started":"2023-05-19T06:56:19.550556Z","shell.execute_reply":"2023-05-19T06:56:19.559308Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"n_epochs = 50\nseq_len = 50\nclip = 0.25\nsaved = False\n\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n\nif saved:\n    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\nelse:\n    best_valid_loss = float('inf')\n\n    for epoch in range(n_epochs):\n        train_loss = train(model, train_data, optimizer, criterion, \n                    batch_size, seq_len, clip, device)\n        valid_loss = evaluate(model, valid_data, criterion, batch_size, \n                    seq_len, device)\n        \n        lr_scheduler.step(valid_loss)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n\n        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:56:20.234947Z","iopub.execute_input":"2023-05-19T06:56:20.235901Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Perplexity: 850.246\n\tValid Perplexity: 2807.658\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Perplexity: 442.436\n\tValid Perplexity: 277.436\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Perplexity: 301.807\n\tValid Perplexity: 227.298\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Perplexity: 243.571\n\tValid Perplexity: 197.576\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Perplexity: 206.231\n\tValid Perplexity: 179.433\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Perplexity: 179.746\n\tValid Perplexity: 167.214\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Perplexity: 159.478\n\tValid Perplexity: 151.486\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d9d3f1478b4cd8aec3eaa57577e98b"}},"metadata":{}}]},{"cell_type":"code","source":"def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n    if seed is not None:\n        torch.manual_seed(seed)\n    model.eval()\n    tokens = tokenizer(prompt)\n    indices = [vocab[t] for t in tokens]\n    batch_size = 1\n    hidden = model.init_hidden(batch_size, device)\n    with torch.no_grad():\n        for i in range(max_seq_len):\n            src = torch.LongTensor([indices]).to(device)\n            prediction, hidden = model(src, hidden)\n            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n            prediction = torch.multinomial(probs, num_samples=1).item()    \n            \n            while prediction == vocab['<unk>']:\n                prediction = torch.multinomial(probs, num_samples=1).item()\n\n            if prediction == vocab['<eos>']:\n                break\n\n            indices.append(prediction)\n\n    itos = vocab.get_itos()\n    tokens = [itos[i] for i in indices]\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:48.743037Z","iopub.status.idle":"2023-05-19T06:50:48.744051Z","shell.execute_reply.started":"2023-05-19T06:50:48.743798Z","shell.execute_reply":"2023-05-19T06:50:48.743823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = 'Think about'\nmax_seq_len = 30\nseed = 0\n\ntemperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\nfor temperature in temperatures:\n    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n                          vocab, device, seed)\n    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:50:48.745714Z","iopub.status.idle":"2023-05-19T06:50:48.746196Z","shell.execute_reply.started":"2023-05-19T06:50:48.745967Z","shell.execute_reply":"2023-05-19T06:50:48.74599Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
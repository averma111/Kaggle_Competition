{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/tiny-language-model-in-pytorch?scriptVersionId=130149757\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torchtext\nimport datasets\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-19T06:35:44.802765Z","iopub.execute_input":"2023-05-19T06:35:44.803557Z","iopub.status.idle":"2023-05-19T06:35:49.877716Z","shell.execute_reply.started":"2023-05-19T06:35:44.803495Z","shell.execute_reply":"2023-05-19T06:35:49.876498Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:36:04.208595Z","iopub.execute_input":"2023-05-19T06:36:04.208996Z","iopub.status.idle":"2023-05-19T06:36:04.222595Z","shell.execute_reply.started":"2023-05-19T06:36:04.208961Z","shell.execute_reply":"2023-05-19T06:36:04.221577Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7b27c4bbb9d0>"},"metadata":{}}]},{"cell_type":"code","source":"dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\nprint(dataset)\nprint(dataset['train'][88]['text'])","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:36:39.507667Z","iopub.execute_input":"2023-05-19T06:36:39.508056Z","iopub.status.idle":"2023-05-19T06:36:47.024567Z","shell.execute_reply.started":"2023-05-19T06:36:39.508024Z","shell.execute_reply":"2023-05-19T06:36:47.023445Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be07c3225d364301a73a60555308ff2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63638dae10be4518bcbd630941356acb"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09312c2f86204cb0b7c204e02f1fb7b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d6cc8d93fd0451699ced68881b55fac"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . Gunsmiths were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen 'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\ntokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \ntokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \nfn_kwargs={'tokenizer': tokenizer})\nprint(tokenized_dataset['train'][88]['tokens'])","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:38:15.861232Z","iopub.execute_input":"2023-05-19T06:38:15.861641Z","iopub.status.idle":"2023-05-19T06:38:20.718534Z","shell.execute_reply.started":"2023-05-19T06:38:15.861607Z","shell.execute_reply":"2023-05-19T06:38:20.717407Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4358 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de4aa9d040fe4d19960c9ab765bdf8e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36718 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f31ee2dfd54087a56d84635fffed19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3760 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a74504bf1004fe79113f21795606019"}},"metadata":{}},{"name":"stdout","text":"['this', 'ammunition', ',', 'and', 'that', 'which', 'i', 'brought', 'with', 'me', ',', 'was', 'rapidly', 'prepared', 'for', 'use', 'at', 'the', 'laboratory', 'established', 'at', 'the', 'little', 'rock', 'arsenal', 'for', 'that', 'purpose', '.', 'as', 'illustrating', 'as', 'the', 'pitiful', 'scarcity', 'of', 'material', 'in', 'the', 'country', ',', 'the', 'fact', 'may', 'be', 'stated', 'that', 'it', 'was', 'found', 'necessary', 'to', 'use', 'public', 'documents', 'of', 'the', 'state', 'library', 'for', 'cartridge', 'paper', '.', 'gunsmiths', 'were', 'employed', 'or', 'conscripted', ',', 'tools', 'purchased', 'or', 'impressed', ',', 'and', 'the', 'repair', 'of', 'the', 'damaged', 'guns', 'i', 'brought', 'with', 'me', 'and', 'about', 'an', 'equal', 'number', 'found', 'at', 'little', 'rock', 'commenced', 'at', 'once', '.', 'but', ',', 'after', 'inspecting', 'the', 'work', 'and', 'observing', 'the', 'spirit', 'of', 'the', 'men', 'i', 'decided', 'that', 'a', 'garrison', '500', 'strong', 'could', 'hold', 'out', 'against', 'fitch', 'and', 'that', 'i', 'would', 'lead', 'the', 'remainder', '-', 'about', '1500', '-', 'to', 'gen', \"'\", 'l', 'rust', 'as', 'soon', 'as', 'shotguns', 'and', 'rifles', 'could', 'be', 'obtained', 'from', 'little', 'rock', 'instead', 'of', 'pikes', 'and', 'lances', ',', 'with', 'which', 'most', 'of', 'them', 'were', 'armed', '.', 'two', 'days', 'elapsed', 'before', 'the', 'change', 'could', 'be', 'effected', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'],min_freq=3) \nvocab.insert_token('<unk>', 0)           \nvocab.insert_token('<eos>', 1)            \nvocab.set_default_index(vocab['<unk>'])   \nprint(len(vocab))                         \nprint(vocab.get_itos()[:10])  ","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:39:52.76308Z","iopub.execute_input":"2023-05-19T06:39:52.763462Z","iopub.status.idle":"2023-05-19T06:39:55.57973Z","shell.execute_reply.started":"2023-05-19T06:39:52.763431Z","shell.execute_reply":"2023-05-19T06:39:55.578593Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"29473\n['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_data(dataset, vocab, batch_size):\n    data = []                                                   \n    for example in dataset:\n        if example['tokens']:                                      \n            tokens = example['tokens'].append('<eos>')             \n            tokens = [vocab[token] for token in example['tokens']] \n            data.extend(tokens)                                    \n    data = torch.LongTensor(data)                                 \n    num_batches = data.shape[0] // batch_size \n    data = data[:num_batches * batch_size]                       \n    data = data.view(batch_size, num_batches)          \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:41:14.204826Z","iopub.execute_input":"2023-05-19T06:41:14.205243Z","iopub.status.idle":"2023-05-19T06:41:14.21306Z","shell.execute_reply.started":"2023-05-19T06:41:14.205208Z","shell.execute_reply":"2023-05-19T06:41:14.212136Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\ntrain_data = get_data(tokenized_dataset['train'], vocab, batch_size)\nvalid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\ntest_data = get_data(tokenized_dataset['test'], vocab, batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:42:00.283398Z","iopub.execute_input":"2023-05-19T06:42:00.284026Z","iopub.status.idle":"2023-05-19T06:42:06.981882Z","shell.execute_reply.started":"2023-05-19T06:42:00.283989Z","shell.execute_reply":"2023-05-19T06:42:06.980928Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, \n                tie_weights):\n                \n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n                    dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n        if tie_weights:\n            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n            self.embedding.weight = self.fc.weight\n        self.init_weights()\n\n    def forward(self, src, hidden):\n        embedding = self.dropout(self.embedding(src))\n        output, hidden = self.lstm(embedding, hidden)          \n        output = self.dropout(output) \n        prediction = self.fc(output)\n        return prediction, hidden\n    \n    def init_weights(self):\n        init_range_emb = 0.1\n        init_range_other = 1/math.sqrt(self.hidden_dim)\n        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n        self.fc.bias.data.zero_()\n        for i in range(self.num_layers):\n            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n\n    def init_hidden(self, batch_size, device):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        return hidden, cell\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:46:07.86639Z","iopub.execute_input":"2023-05-19T06:46:07.866828Z","iopub.status.idle":"2023-05-19T06:46:07.878405Z","shell.execute_reply.started":"2023-05-19T06:46:07.866793Z","shell.execute_reply":"2023-05-19T06:46:07.877377Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   def detach_hidden(self, hidden):\n        hidden, cell = hidden\n        hidden = hidden.detach()\n        cell = cell.detach()\n        return hidden, cell","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:46:09.355002Z","iopub.execute_input":"2023-05-19T06:46:09.355372Z","iopub.status.idle":"2023-05-19T06:46:09.360845Z","shell.execute_reply.started":"2023-05-19T06:46:09.355342Z","shell.execute_reply":"2023-05-19T06:46:09.359734Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 1024             # 400 in the paper\nhidden_dim = 1024                # 1150 in the paper\nnum_layers = 2                   # 3 in the paper\ndropout_rate = 0.65              \ntie_weights = True                  \nlr = 1e-3                        # They used 30 and a different optimizer","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:46:09.887135Z","iopub.execute_input":"2023-05-19T06:46:09.887552Z","iopub.status.idle":"2023-05-19T06:46:09.893759Z","shell.execute_reply.started":"2023-05-19T06:46:09.887504Z","shell.execute_reply":"2023-05-19T06:46:09.892587Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'The model has {num_params:,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:46:10.563357Z","iopub.execute_input":"2023-05-19T06:46:10.563764Z","iopub.status.idle":"2023-05-19T06:46:11.985049Z","shell.execute_reply.started":"2023-05-19T06:46:10.56372Z","shell.execute_reply":"2023-05-19T06:46:11.983787Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"The model has 47,003,425 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_batch(data, seq_len, num_batches, idx):\n    src = data[:, idx:idx+seq_len]                   \n    target = data[:, idx+1:idx+seq_len+1]             \n    return src, target","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:47:10.840197Z","iopub.execute_input":"2023-05-19T06:47:10.84059Z","iopub.status.idle":"2023-05-19T06:47:10.845995Z","shell.execute_reply.started":"2023-05-19T06:47:10.840558Z","shell.execute_reply":"2023-05-19T06:47:10.844844Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n    \n    epoch_loss = 0\n    model.train()\n    # drop all batches that are not a multiple of seq_len\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = model.init_hidden(batch_size, device)\n    \n    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n        optimizer.zero_grad()\n        hidden = model.detach_hidden(hidden)\n\n        src, target = get_batch(data, seq_len, num_batches, idx)\n        src, target = src.to(device), target.to(device)\n        batch_size = src.shape[0]\n        prediction, hidden = model(src, hidden)               \n\n        prediction = prediction.reshape(batch_size * seq_len, -1)   \n        target = target.reshape(-1)\n        loss = criterion(prediction, target)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:47:32.570326Z","iopub.execute_input":"2023-05-19T06:47:32.570715Z","iopub.status.idle":"2023-05-19T06:47:32.578498Z","shell.execute_reply.started":"2023-05-19T06:47:32.570682Z","shell.execute_reply":"2023-05-19T06:47:32.577588Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, data, criterion, batch_size, seq_len, device):\n\n    epoch_loss = 0\n    model.eval()\n    num_batches = data.shape[-1]\n    data = data[:, :num_batches - (num_batches -1) % seq_len]\n    num_batches = data.shape[-1]\n\n    hidden = model.init_hidden(batch_size, device)\n\n    with torch.no_grad():\n        for idx in range(0, num_batches - 1, seq_len):\n            hidden = model.detach_hidden(hidden)\n            src, target = get_batch(data, seq_len, num_batches, idx)\n            src, target = src.to(device), target.to(device)\n            batch_size= src.shape[0]\n\n            prediction, hidden = model(src, hidden)\n            prediction = prediction.reshape(batch_size * seq_len, -1)\n            target = target.reshape(-1)\n\n            loss = criterion(prediction, target)\n            epoch_loss += loss.item() * seq_len\n    return epoch_loss / num_batches","metadata":{"execution":{"iopub.status.busy":"2023-05-19T06:47:56.785198Z","iopub.execute_input":"2023-05-19T06:47:56.785579Z","iopub.status.idle":"2023-05-19T06:47:56.793961Z","shell.execute_reply.started":"2023-05-19T06:47:56.785545Z","shell.execute_reply":"2023-05-19T06:47:56.792751Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"n_epochs = 50\nseq_len = 50\nclip = 0.25\nsaved = True\n\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n\nif saved:\n    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\nelse:\n    best_valid_loss = float('inf')\n\n    for epoch in range(n_epochs):\n        train_loss = train(model, train_data, optimizer, criterion, \n                    batch_size, seq_len, clip, device)\n        valid_loss = evaluate(model, valid_data, criterion, batch_size, \n                    seq_len, device)\n        \n        lr_scheduler.step(valid_loss)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n\n        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n    if seed is not None:\n        torch.manual_seed(seed)\n    model.eval()\n    tokens = tokenizer(prompt)\n    indices = [vocab[t] for t in tokens]\n    batch_size = 1\n    hidden = model.init_hidden(batch_size, device)\n    with torch.no_grad():\n        for i in range(max_seq_len):\n            src = torch.LongTensor([indices]).to(device)\n            prediction, hidden = model(src, hidden)\n            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n            prediction = torch.multinomial(probs, num_samples=1).item()    \n            \n            while prediction == vocab['<unk>']:\n                prediction = torch.multinomial(probs, num_samples=1).item()\n\n            if prediction == vocab['<eos>']:\n                break\n\n            indices.append(prediction)\n\n    itos = vocab.get_itos()\n    tokens = [itos[i] for i in indices]\n    return tokens\nview rawinference.py hosted with ‚ù§ by","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = 'Think about'\nmax_seq_len = 30\nseed = 0\n\ntemperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\nfor temperature in temperatures:\n    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n                          vocab, device, seed)\n    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')","metadata":{},"execution_count":null,"outputs":[]}]}
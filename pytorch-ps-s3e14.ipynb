{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/pytorch-ps-s3e14?scriptVersionId=128570093\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-06T00:35:11.93544Z","iopub.execute_input":"2023-05-06T00:35:11.935846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global Setting","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\npd.set_option('mode.chained_assignment',None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assigning the directory and file paths","metadata":{}},{"cell_type":"code","source":"ROOT_PATH='/kaggle/input/playground-series-s3e14'\ntrain_file = 'train.csv'\ntest_file = 'test.csv'\nsample = 'sample_submission.csv'\noriginal = '/kaggle/input/wild-blueberry-yield-prediction/Data in Brief/Data in Brief/WildBlueberryPollinationSimulationData.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the train data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(ROOT_PATH+'/'+train_file)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop('id',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(ROOT_PATH+'/'+test_file)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_df = pd.read_csv(original)\noriginal_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_df.drop('Row#',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_full = pd.concat([train, original_df])\nfor col in original_df.columns:\n    df_full[col] = df_full[col].astype('float64')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the summary function","metadata":{}},{"cell_type":"code","source":"def summary(text, df):\n    print(f'{text} shape: {df.shape}')\n    summ = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summ['null'] = df.isnull().sum()\n    summ['unique'] = df.nunique()\n    summ['min'] = df.min()\n    summ['median'] = df.median()\n    summ['max'] = df.max()\n    summ['mean'] = df.mean()\n    summ['std'] = df.std()\n    summ['inf'] = np.isinf(df).sum().sum()\n    summ['duplicate'] = df.duplicated().sum()\n    return summ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Summary of the trained data","metadata":{}},{"cell_type":"code","source":"summary('train',df_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* No null values. We therefore dont need to use imputation\n* Categorical data ==> No Categorical data\n* Data types are all float values excluding the target (integer)\n* Data is reasonably small with only 15289 datapoints\n* Duplicates: 7 duplicate","metadata":{}},{"cell_type":"code","source":"df_full = df_full.drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary('full',df_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_full[\"fruit_seed\"] = df_full[\"fruitset\"] * df_full[\"seeds\"]\ntest[\"fruit_seed\"] = test[\"fruitset\"] * test[\"seeds\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ## Pairplot of the train dataset\n\n* Distribution looks fairely normal with -negative skewness","metadata":{}},{"cell_type":"code","source":"sns.displot(df_full['yield'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Train vs Test data\n\n* The distribution of Test and Train datasets seem to align","metadata":{}},{"cell_type":"code","source":"y = df_full['yield']\ndf_full.drop(columns=['yield'],axis=1,inplace=True)\nX = df_full.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Train and Test look synonimous","metadata":{}},{"cell_type":"code","source":"\nfig,ax = plt.subplots(int(np.ceil(len(X.columns)/4)),4, figsize = (30,25))\nfor i,col in enumerate(X.columns):\n    ax = np.ravel(ax)\n\n    sns.kdeplot(x= X[col] , label = 'Train', ax = ax[i])\n    sns.kdeplot(x= test[col], label = 'Test', ax = ax[i] )\n    \n    ax[i].legend()\n    ax[i].set_title(f\"col\")\n\nplt.suptitle(\"Distribution of Train vs Test Dataset\",fontsize = 30)\nplt.tight_layout(pad =3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrix\n\n* Dataset looks highly correlated with target field","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (25,12))\n\ncorr = train.corr()\nupper_triangle = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr,vmin = -1, vmax = 1, cmap = \"Spectral\", annot = True, mask = upper_triangle)\nplt.title(\"Correlation of all features and target\", fontsize= 18)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outlier Analaysis","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(int(np.ceil(len(X.columns)/4)),4,figsize = (30,15))\nax = np.ravel(ax)\n\nfor i,col in enumerate(X.columns):\n    sns.boxplot(ax = ax[i], x = X[col], color= \"red\")\n\nfig.suptitle(\"Box plots of all data \",fontsize = 20)\nplt.tight_layout(pad=3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardize the numerical features in the dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\nscaler = StandardScaler()\nnumerical_cols = list(X.select_dtypes(include=['int','float']))\nX_numerical = scaler.fit_transform(X[numerical_cols].values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert target to numpy array","metadata":{}},{"cell_type":"code","source":"y = y.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the train data into train test datasets for modelling","metadata":{}},{"cell_type":"code","source":"# Split the data into training and test \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_numerical,y,test_size=0.2,random_state=42)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generic Hyper parameters","metadata":{}},{"cell_type":"code","source":"# Model Parameters \nEPOCHS = 100\nBATCH_SIZE = 32\nLEARNING_RATE = 0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the Dataset and Dataloader classes for test and train","metadata":{}},{"cell_type":"code","source":"# Define the DataLoader for train and test data\nfrom torch.utils.data import Dataset, DataLoader\n# Train Data\nclass TrainData(Dataset):\n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n    \n# Test Data   \nclass TestData(Dataset):\n    \n    def __init__(self, X_data):\n        self.X_data = X_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting the dataset into torch tensor format","metadata":{}},{"cell_type":"code","source":"# Instantiate the Train and Test data class\nimport torch\ntrain_data = TrainData(torch.tensor(data=X_train,dtype=torch.float32,requires_grad=True),\n                       torch.tensor(data=y_train,dtype=torch.float32,requires_grad=True))\n\ntest_data = TrainData(torch.tensor(data=X_test,dtype=torch.float32,requires_grad=True),\n                       torch.tensor(data=y_test,dtype=torch.float32,requires_grad=True))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the iterator Dataloader","metadata":{}},{"cell_type":"code","source":"# Initialize the DataLoader \ntrain_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the pytorch regression class","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass RegressionBlueBerryNNet(torch.nn.Module):\n    def __init__(self,input_features):\n        super(RegressionBlueBerryNNet,self).__init__()\n        # Number of input features is 16.\n        self.input_layer = torch.nn.Linear(input_features,1)\n        #self.dense_layer = torch.nn.Linear(32,1)\n        #self.output_layer = torch.nn.Linear(32,1)\n        \n        self.relu = torch.nn.ReLU()\n        #self.dropout = torch.nn.Dropout(p=0.1)\n        #self.batchnorm_1 = torch.nn.BatchNorm1d(32)\n        #self.batchnorm_2 = torch.nn.BatchNorm1d(64)\n        \n    \n    def forward(self,inputs):\n        x = self.relu(self.input_layer(inputs))\n        #x = self.input_layer(inputs)\n        #x = self.batchnorm_1(x)\n        #x = self.dense_layer(x)\n        #x = self.batchnorm_2(x)\n        #x = self.dropout(x)\n        #x = self.output_layer(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping():\n    def __init__(self, patience=5, min_delta=0.1,test_loss=None):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, test_loss):\n        if self.best_loss == None:\n            self.best_loss = test_loss\n        elif self.best_loss - test_loss > self.min_delta:\n            self.best_loss = test_loss\n            self.counter = 0\n        elif self.best_loss - test_loss < self.min_delta:\n            self.counter += 1\n            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                print('INFO: Early stopping')\n                self.early_stop = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the code to run both on CPU and GPU","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiating the model","metadata":{}},{"cell_type":"code","source":"model = RegressionBlueBerryNNet(X_train.shape[1])\nmodel.to(device)\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the Loss and Optimizer ","metadata":{}},{"cell_type":"code","source":"# Define the loss and optimizer \ncriterion = torch.nn.L1Loss() ## Mean Absolute Error\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE,momentum=0.9)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Traning the Model :)","metadata":{}},{"cell_type":"code","source":"# Tain the model\nfrom tqdm.notebook import tqdm\n#early_stopper = EarlyStopper(patience=3, min_delta=10)\nmodel.train()\nfor e in tqdm(range(1, EPOCHS+1)):\n    epoch_loss = 0.0\n    epoch_loss_test = 0.0\n    \n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        y_pred = model(X_batch)\n        \n        loss = criterion(y_pred, y_batch.unsqueeze(1))\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    \n    for X_batch_test, y_batch_test in test_loader :\n        X_batch_test, y_batch_test = X_batch_test.to(device), y_batch_test.to(device)\n        \n        y_pred_test = model(X_batch_test)\n        loss_test = criterion(y_pred_test, y_batch_test.unsqueeze(1))\n        epoch_loss_test += loss_test.item()\n        \n    \n        \n    if e%10 == 0:\n        print(f'Epoch {e+0:03}: | Train_Loss: {epoch_loss/len(train_loader):.5f} | Test_Loss: {epoch_loss_test/len(test_loader):.5f} ')\n    \n    earrly_stopping = EarlyStopping(epoch_loss_test)\n    if earrly_stopping.early_stop:\n        break\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Executing model on test data","metadata":{}},{"cell_type":"markdown","source":"### Checking the summary of the test data","metadata":{}},{"cell_type":"code","source":"summary('test',test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Test variable ","metadata":{}},{"cell_type":"code","source":"X_val=test.loc[:, test.columns != 'id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Filtering the continous columns","metadata":{}},{"cell_type":"code","source":"numerical_cols_test = list(X_val.select_dtypes(include=['int','float']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardizing the test data as train data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\nscaler = StandardScaler()\nX_numerical_test = scaler.fit_transform(X_val[numerical_cols_test].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the Test Dataset class","metadata":{}},{"cell_type":"code","source":"# Test Data   \nclass TestingData(Dataset):\n    \n    def __init__(self, X_test_data):\n        self.X_test_data = X_test_data\n        \n    def __getitem__(self, index):\n        return self.X_test_data[index]\n        \n    def __len__ (self):\n        return len(self.X_test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the DataLoader class","metadata":{}},{"cell_type":"code","source":"test_data_model = TestingData(torch.tensor(data=X_numerical_test,dtype=torch.float32))\ntest_data_model_loader = DataLoader(dataset=test_data_model, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating the yield values for test data","metadata":{}},{"cell_type":"code","source":"# Validating the model on test data\nyield_target = []\nmodel.eval()\nwith torch.no_grad():\n    for X_batch_test in test_data_model_loader:\n        X_batch_test = X_batch_test.to(device)\n        y_test_pred = model(X_batch_test)\n        y_pred_tag = torch.round(y_test_pred)\n        yield_target.append(y_pred_tag.cpu().numpy())\n\nyield_target = [a.squeeze().tolist() for a in yield_target]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the submission file","metadata":{}},{"cell_type":"code","source":"yield_submission=[]\nfor col in yield_target:\n    for val in col:\n        yield_submission.append(val)\n#sp_submission\ndf_test = pd.DataFrame(data={'id': test['id'],'yield': yield_submission})\ndf_test.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
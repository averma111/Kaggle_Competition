{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/averma111/pytorch-ps3e15?scriptVersionId=129878852\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"%%capture \n!pip install optuna","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:33:57.417067Z","iopub.execute_input":"2023-05-17T05:33:57.417451Z","iopub.status.idle":"2023-05-17T05:34:12.520376Z","shell.execute_reply.started":"2023-05-17T05:33:57.41742Z","shell.execute_reply":"2023-05-17T05:34:12.51881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport torch\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nwarnings.filterwarnings('ignore')\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nsns.set_style(\"darkgrid\")\npd.set_option('mode.chained_assignment',None)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:12.523197Z","iopub.execute_input":"2023-05-17T05:34:12.523696Z","iopub.status.idle":"2023-05-17T05:34:16.859568Z","shell.execute_reply.started":"2023-05-17T05:34:12.523651Z","shell.execute_reply":"2023-05-17T05:34:16.85856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataframe(path):\n    df=pd.read_csv(path)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:16.860994Z","iopub.execute_input":"2023-05-17T05:34:16.861647Z","iopub.status.idle":"2023-05-17T05:34:16.867806Z","shell.execute_reply.started":"2023-05-17T05:34:16.861602Z","shell.execute_reply":"2023-05-17T05:34:16.866161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = get_dataframe('/kaggle/input/playground-series-s3e15/data.csv')\noriginal = get_dataframe('/kaggle/input/predicting-heat-flux/Data_CHF_Zhao_2020_ATE.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:16.870947Z","iopub.execute_input":"2023-05-17T05:34:16.871347Z","iopub.status.idle":"2023-05-17T05:34:16.982139Z","shell.execute_reply.started":"2023-05-17T05:34:16.871312Z","shell.execute_reply":"2023-05-17T05:34:16.981112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summary(text, df):\n    print(f'{text} shape: {df.shape}')\n    summ = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summ['null'] = df.isnull().sum()\n    summ['unique'] = df.nunique()\n    summ['min'] = df.min()\n    summ['median'] = df.median()\n    summ['max'] = df.max()\n    summ['mean'] = df.mean()\n    summ['std'] = df.std()\n    summ['duplicate'] = df.duplicated().sum()\n    return summ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:16.983586Z","iopub.execute_input":"2023-05-17T05:34:16.984707Z","iopub.status.idle":"2023-05-17T05:34:16.992059Z","shell.execute_reply.started":"2023-05-17T05:34:16.984671Z","shell.execute_reply":"2023-05-17T05:34:16.990866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary('data',data)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:16.99381Z","iopub.execute_input":"2023-05-17T05:34:16.994376Z","iopub.status.idle":"2023-05-17T05:34:17.248868Z","shell.execute_reply.started":"2023-05-17T05:34:16.994346Z","shell.execute_reply":"2023-05-17T05:34:17.247978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary('original',original)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:17.250136Z","iopub.execute_input":"2023-05-17T05:34:17.250666Z","iopub.status.idle":"2023-05-17T05:34:17.296506Z","shell.execute_reply.started":"2023-05-17T05:34:17.250612Z","shell.execute_reply":"2023-05-17T05:34:17.295193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data,x='x_e_out [-]',color='r')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:17.29815Z","iopub.execute_input":"2023-05-17T05:34:17.298852Z","iopub.status.idle":"2023-05-17T05:34:18.201574Z","shell.execute_reply.started":"2023-05-17T05:34:17.29882Z","shell.execute_reply":"2023-05-17T05:34:18.200319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(original,x='x_e_out [-]',color='b')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:18.203829Z","iopub.execute_input":"2023-05-17T05:34:18.204753Z","iopub.status.idle":"2023-05-17T05:34:18.67809Z","shell.execute_reply.started":"2023-05-17T05:34:18.204709Z","shell.execute_reply":"2023-05-17T05:34:18.676935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_numerical_features(df):\n    numerical_feature = df.select_dtypes(include=['float64'])\n    return numerical_feature\n\nnumerical_features = get_numerical_features(data)\nnumerical_features.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:18.683527Z","iopub.execute_input":"2023-05-17T05:34:18.685954Z","iopub.status.idle":"2023-05-17T05:34:18.707837Z","shell.execute_reply.started":"2023-05-17T05:34:18.685916Z","shell.execute_reply":"2023-05-17T05:34:18.706613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_categorical_features(df):\n    categorical_features = df.select_dtypes(include=['object'])\n    return categorical_features\n\ncategorical_features = get_categorical_features(data)\ncategorical_features.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:18.709526Z","iopub.execute_input":"2023-05-17T05:34:18.710509Z","iopub.status.idle":"2023-05-17T05:34:18.72424Z","shell.execute_reply.started":"2023-05-17T05:34:18.710467Z","shell.execute_reply":"2023-05-17T05:34:18.722775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_numerical_histogram():\n    fig, ax = plt.subplots(7, 1, figsize = (5, 15), dpi = 90)\n    ax = ax.flatten()\n\n    for i, column in enumerate(numerical_features):\n        sns.histplot(data[column], ax=ax[i], color='r')\n        sns.histplot(original[column], ax=ax[i], color='b')\n    \n        ax[i].set_title(f'{column} Distribution', size = 5)\n        ax[i].set_xlabel(None)\n        ax[i].set_ylabel(None)\n    \n    fig.suptitle('Distribution of Numerical Feature', fontsize = 8)\n    plt.tight_layout()\n    \nplot_numerical_histogram()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:18.725889Z","iopub.execute_input":"2023-05-17T05:34:18.726698Z","iopub.status.idle":"2023-05-17T05:34:23.543195Z","shell.execute_reply.started":"2023-05-17T05:34:18.726658Z","shell.execute_reply":"2023-05-17T05:34:23.541496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_categorical_data(df,column_name,palette,dataset_name):\n    fig, ax = plt.subplots(1, 1, figsize = (12, 4))\n    #ax = ax.flatten()\n    sns.countplot(data = df, y = column_name, ax = ax, palette = palette, \n                  order = data[column_name].value_counts().index)\n    ax.yaxis.label.set_size(20)\n    plt.yticks(fontsize = 12)\n    ax.set_xlabel('Count', fontsize = 20)\n    ax.set_ylabel(None)\n    plt.xticks(fontsize = 12)\n\n    fig.suptitle(f'{column_name.title()} in {dataset_name} Dataset', fontsize = 15, fontweight = 'bold')\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:23.544931Z","iopub.execute_input":"2023-05-17T05:34:23.545371Z","iopub.status.idle":"2023-05-17T05:34:23.555017Z","shell.execute_reply.started":"2023-05-17T05:34:23.54534Z","shell.execute_reply":"2023-05-17T05:34:23.553783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_categorical_data(data,'author','flare','competition')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:23.556743Z","iopub.execute_input":"2023-05-17T05:34:23.557208Z","iopub.status.idle":"2023-05-17T05:34:24.028656Z","shell.execute_reply.started":"2023-05-17T05:34:23.557165Z","shell.execute_reply":"2023-05-17T05:34:24.027176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_categorical_data(data,'geometry','flare','competition')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:24.030451Z","iopub.execute_input":"2023-05-17T05:34:24.030917Z","iopub.status.idle":"2023-05-17T05:34:24.405925Z","shell.execute_reply.started":"2023-05-17T05:34:24.030874Z","shell.execute_reply":"2023-05-17T05:34:24.404767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_categorical_data(original,'author','ch:s=.25,rot=-.25','original')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:24.407267Z","iopub.execute_input":"2023-05-17T05:34:24.40758Z","iopub.status.idle":"2023-05-17T05:34:24.801228Z","shell.execute_reply.started":"2023-05-17T05:34:24.407553Z","shell.execute_reply":"2023-05-17T05:34:24.800116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_categorical_data(original,'geometry','ch:s=.25,rot=-.25','original')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:24.802699Z","iopub.execute_input":"2023-05-17T05:34:24.803048Z","iopub.status.idle":"2023-05-17T05:34:25.143278Z","shell.execute_reply.started":"2023-05-17T05:34:24.803018Z","shell.execute_reply":"2023-05-17T05:34:25.142112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_correlation(dataset, column_name,cmap):\n    corr = dataset.corr(method = 'kendall')\n    plt.figure(figsize = (10, 10), dpi = 90)\n    mask = np.zeros_like(corr)\n    mask[np.triu_indices_from(mask)] = True\n    sns.heatmap(corr, mask = mask, cmap = cmap, annot = True, annot_kws = {'size' : 12})\n    plt.title(f'{column_name} Dataset Correlation Matrix\\n', fontsize = 15, weight = 'bold')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:25.144673Z","iopub.execute_input":"2023-05-17T05:34:25.14501Z","iopub.status.idle":"2023-05-17T05:34:25.152644Z","shell.execute_reply.started":"2023-05-17T05:34:25.144981Z","shell.execute_reply":"2023-05-17T05:34:25.151382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_correlation(data[numerical_features.columns],'Competition','flare')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:25.153969Z","iopub.execute_input":"2023-05-17T05:34:25.154284Z","iopub.status.idle":"2023-05-17T05:34:26.023965Z","shell.execute_reply.started":"2023-05-17T05:34:25.154257Z","shell.execute_reply":"2023-05-17T05:34:26.022713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_correlation(original[numerical_features.columns],'Original','coolwarm')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:34:26.026019Z","iopub.execute_input":"2023-05-17T05:34:26.026447Z","iopub.status.idle":"2023-05-17T05:34:26.565193Z","shell.execute_reply.started":"2023-05-17T05:34:26.026409Z","shell.execute_reply":"2023-05-17T05:34:26.564344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_train_test_data(data,original):\n    \n    feature = data[['pressure [MPa]', 'mass_flux [kg/m2-s]', 'chf_exp [MW/m2]', 'length [mm]']].copy()\n    feature['mass_flux_missing'] = np.where(feature['mass_flux [kg/m2-s]'].isnull(), 1, 0)\n    feature['pressure_missing'] = np.where(feature['pressure [MPa]'].isnull(), 1, 0)\n    feature['chf_missing'] = np.where(feature['chf_exp [MW/m2]'].isnull(), 1, 0)\n    feature['generated'] = 1\n\n    feature_org = original[['pressure [MPa]', 'mass_flux [kg/m2-s]', 'chf_exp [MW/m2]', 'length [mm]']].copy()\n    feature_org['mass_flux_missing'] = np.where(feature_org['mass_flux [kg/m2-s]'].isnull(), 1, 0)\n    feature_org['pressure_missing'] = np.where(feature_org['pressure [MPa]'].isnull(), 1, 0)\n    feature_org['chf_missing'] = np.where(feature_org['chf_exp [MW/m2]'].isnull(), 1, 0)\n    feature_org['generated'] = 0\n\n    label = data['x_e_out [-]']\n    label_org = original['x_e_out [-]']\n\n    X = pd.concat([feature, feature_org], axis = 0).reset_index(drop = True)\n    y = pd.concat([label, label_org], axis = 0).reset_index(drop = True)\n\n    X.columns = ['pressure', 'mass_flux', 'chf_exp', 'length', 'mass_flux_missing', 'pressure_missing', 'chf_missing', 'generated']\n\n    test = X[y.isnull()]\n    X = X[~y.isnull()]\n    y = y[~y.isnull()]\n    \n    return X,y,test\n\n\nX,y,test=generate_train_test_data(data,original)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T05:50:26.71019Z","iopub.execute_input":"2023-05-17T05:50:26.710733Z","iopub.status.idle":"2023-05-17T05:50:26.748282Z","shell.execute_reply.started":"2023-05-17T05:50:26.710696Z","shell.execute_reply":"2023-05-17T05:50:26.74689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(X,y):\n    return train_test_split(X,y.to_numpy(),test_size=0.3,random_state=42)\nX_train,X_val,y_train,y_val  = split_data(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:09:59.557114Z","iopub.execute_input":"2023-05-17T06:09:59.557568Z","iopub.status.idle":"2023-05-17T06:09:59.570934Z","shell.execute_reply.started":"2023-05-17T06:09:59.557537Z","shell.execute_reply":"2023-05-17T06:09:59.569589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_dataset(X_train,X_val):\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    return X_train,X_val\n\nX_train,X_val=normalize_dataset(X_train,X_val)\nprint(X_train.shape,X_val.shape,y_train.shape,y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:10:09.179277Z","iopub.execute_input":"2023-05-17T06:10:09.179902Z","iopub.status.idle":"2023-05-17T06:10:09.203046Z","shell.execute_reply.started":"2023-05-17T06:10:09.179849Z","shell.execute_reply":"2023-05-17T06:10:09.201765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_torch(value):\n    return torch.tensor(data=value,dtype=torch.float32,requires_grad=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:10:18.098118Z","iopub.execute_input":"2023-05-17T06:10:18.098516Z","iopub.status.idle":"2023-05-17T06:10:18.105661Z","shell.execute_reply.started":"2023-05-17T06:10:18.098487Z","shell.execute_reply":"2023-05-17T06:10:18.103913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FluxData(Dataset):\n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n            return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\nX_data = convert_to_torch(X_train)\ny_data = convert_to_torch(y_train)\nX_val = convert_to_torch(X_val)\ny_val = convert_to_torch(y_val)\ntrain_data = FluxData(X_data,y_data)\ntest_data = FluxData(X_val,y_val)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:10:19.327324Z","iopub.execute_input":"2023-05-17T06:10:19.328823Z","iopub.status.idle":"2023-05-17T06:10:19.337476Z","shell.execute_reply.started":"2023-05-17T06:10:19.328778Z","shell.execute_reply":"2023-05-17T06:10:19.336372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RegressionFluxModel(torch.nn.Module):\n    def training_step(self,batch):\n        features,label = batch\n        output = self(features)\n        loss = torch.sqrt(torch.nn.MSELoss(output,label.unsqueeze(1)))\n        return loss\n    \n    def validation_step(self,batch):\n        features, labels = batch \n        output = self(features)                    \n        loss = torch.sqrt(torch.nn.MSELoss(output,label.unsqueeze(1)))       \n        return {'Validation_loss': loss.detach()}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['Validation_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   \n        return {'Validation_loss': epoch_loss.item()}\n    \n    \n    def epoch_end(self, epoch, result):\n        if epoch %10 ==0:\n            print(\"Epoch [{}], Train_loss: {:.5f}, Validation_loss: {:.5f}\".format(\n                (epoch+10), result['train_loss'], result['Validation_loss']))\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:22.208837Z","iopub.execute_input":"2023-05-17T06:33:22.209214Z","iopub.status.idle":"2023-05-17T06:33:22.219061Z","shell.execute_reply.started":"2023-05-17T06:33:22.209185Z","shell.execute_reply":"2023-05-17T06:33:22.218179Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"class RegressionFluxNNet(RegressionFluxModel):\n    def __init__(self,input_features):\n        super().__init__()\n        self.network = torch.nn.Sequential(\n        torch.nn.Linear(input_features,1),\n        torch.nn.LeakyReLU()\n        )\n        \n    def forward(self,inputs):\n        return self.network(inputs)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:23.041168Z","iopub.execute_input":"2023-05-17T06:33:23.041605Z","iopub.status.idle":"2023-05-17T06:33:23.048376Z","shell.execute_reply.started":"2023-05-17T06:33:23.04157Z","shell.execute_reply":"2023-05-17T06:33:23.046951Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:23.84178Z","iopub.execute_input":"2023-05-17T06:33:23.842175Z","iopub.status.idle":"2023-05-17T06:33:23.848038Z","shell.execute_reply.started":"2023-05-17T06:33:23.842143Z","shell.execute_reply":"2023-05-17T06:33:23.847016Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"model = RegressionFluxNNet(X_train.shape[1])\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:24.858215Z","iopub.execute_input":"2023-05-17T06:33:24.859147Z","iopub.status.idle":"2023-05-17T06:33:24.866403Z","shell.execute_reply.started":"2023-05-17T06:33:24.859114Z","shell.execute_reply":"2023-05-17T06:33:24.865602Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"RegressionFluxNNet(\n  (network): Sequential(\n    (0): Linear(in_features=8, out_features=1, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 32\nEPOCHS = 100\nLEARNING_RATE = 0.1\nMOMENTUM = 0.9\nOPT_FUNC= torch.optim.SGD","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:25.601226Z","iopub.execute_input":"2023-05-17T06:33:25.601882Z","iopub.status.idle":"2023-05-17T06:33:25.609042Z","shell.execute_reply.started":"2023-05-17T06:33:25.601842Z","shell.execute_reply":"2023-05-17T06:33:25.60771Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def get_dataloaders(dataset_type,batch,shuffle):\n    if shuffle:\n         return DataLoader(dataset=dataset_type, batch_size=batch, shuffle=True)\n    else:\n        return DataLoader(dataset=dataset_type, batch_size=batch,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:26.399426Z","iopub.execute_input":"2023-05-17T06:33:26.400086Z","iopub.status.idle":"2023-05-17T06:33:26.405434Z","shell.execute_reply.started":"2023-05-17T06:33:26.400047Z","shell.execute_reply":"2023-05-17T06:33:26.404615Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, test_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in test_loader]\n    return model.validation_epoch_end(outputs)\n\n  \ndef fit(epochs, lr, model, train_loader, test_loader, opt_func):\n    model.train()\n    history = []\n    optimizer = opt_func(model.parameters(),lr,MOMENTUM)\n    for epoch in tqdm(range(epochs)): \n        train_losses = []\n        for batch in train_loader:\n            optimizer.zero_grad()\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss)\n            \n        result = evaluate(model, test_loader)\n        result['Train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    print('Training is completed!!')\n    return history","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:27.124476Z","iopub.execute_input":"2023-05-17T06:33:27.125666Z","iopub.status.idle":"2023-05-17T06:33:27.13576Z","shell.execute_reply.started":"2023-05-17T06:33:27.125599Z","shell.execute_reply":"2023-05-17T06:33:27.134571Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"train_dataloader = get_dataloaders(train_data,BATCH_SIZE,True)\ntest_dataloader = get_dataloaders(test_data,BATCH_SIZE,False)\nhistory = fit(EPOCHS, LEARNING_RATE, model, train_dataloader, test_dataloader,OPT_FUNC)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T06:33:27.842858Z","iopub.execute_input":"2023-05-17T06:33:27.843631Z","iopub.status.idle":"2023-05-17T06:33:28.07365Z","shell.execute_reply.started":"2023-05-17T06:33:27.843583Z","shell.execute_reply":"2023-05-17T06:33:28.07197Z"},"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34c8be1ac6a437595010fda38044efd"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[80], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m get_dataloaders(train_data,BATCH_SIZE,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m get_dataloaders(test_data,BATCH_SIZE,\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mOPT_FUNC\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[79], line 16\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, test_loader, opt_func)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n","Cell \u001b[0;32mIn[73], line 5\u001b[0m, in \u001b[0;36mRegressionFluxModel.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m      3\u001b[0m features,label \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(features)\n\u001b[0;32m----> 5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:533\u001b[0m, in \u001b[0;36mMSELoss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[1;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n","\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"],"ename":"RuntimeError","evalue":"Boolean value of Tensor with more than one value is ambiguous","output_type":"error"}]},{"cell_type":"code","source":"def plot_losses(history):\n    \"\"\" Plot the losses in each epoch\"\"\"\n    train_losses = [x.get('train_loss') for x in history]\n    test_losses = [x['test_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(test_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Testing'])\n    plt.title('Loss vs. No. of epochs');","metadata":{},"execution_count":null,"outputs":[]}]}